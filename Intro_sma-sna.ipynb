{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img alt=\"\" src=\"images/0_Cover.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">Social Media and Network Analysis</font></center>\n",
    "\n",
    "<h2 id=\"(C)-Taufik-Sutanto---2019\" style=\"text-align: center;\">(C) Taufik Sutanto - 2019</h2>\n",
    "<h2 id=\"tau-data-Indonesia-~-https://tau-data.id\" style=\"text-align: center;\">tau-data Indonesia ~ <a href=\"https://tau-data.id\" target=\"_blank\"><span style=\"color: #0009ff;\">https://tau-data.id</span></a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes and Disclaimer\n",
    "\n",
    "* This notebook is part of the free (open knowledge) eLearning course at: https://tau-data.id/courses/\n",
    "* Some images are taken from several resources, we respect those images ownerships and put a reference/citation from where it is originated. Nevertheless, sometimes we are having trouble to find the origin of the image(s). If you are the owner of the image and would like the image taken-out (or want the citation to be revised) from this open knowledge course resources please contact us here with the details: https://tau-data.id/contact/  \n",
    "* Unless stated otherwise, in general tau-data permit its resources to be copied and-or modified for non-commercial purposes. With condition proper acknowledgement/citation is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Outline:</font>\n",
    "\n",
    "* Pendahuluan Social Media Analytics\n",
    "* Crawling VS Scrapping VS Streaming\n",
    "* Text Analytic (visualisasi data Text)\n",
    "* Pendahuluan Sentiment Analysis\n",
    "* Topic Modelling\n",
    "* Pendahuluan Social Network Analysis\n",
    "* Centrality Analysis\n",
    "* Graph Partitioning and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we begin\n",
    "\n",
    "* Sebaiknya saat ini login ke twitter\n",
    "* Daftarkan no telp (jika belum)\n",
    "* Daftar sebagai twitter developer ke https://developer.twitter.com/en/apps\n",
    "* Daftarkan sebuah Apps, jika ditanya oleh twitter, sampaikan Apps untuk kebutuhan riset/akademis.\n",
    "\n",
    "<h2><img alt=\"\" src=\"images/6_Creating_API_Keys.png\" style=\"width: 854px ; height: 444px\" /></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Analytics (SMA)\n",
    "\n",
    "<h3 id=\"SMA-adalah-sebuah-proses-pengumpulan-data-dari-media-sosial-dan-analisanya-untuk-mendapatkan-'insights'-atau-informasi-berharga-untuk-suatu-tujuan-tertentu-(definisi-adopted-dari-Gartner*)\">SMA adalah sebuah proses pengumpulan data dari media sosial dan analisanya untuk mendapatkan &#39;insights&#39; atau informasi berharga untuk suatu tujuan tertentu (definisi diadopsi dari Gartner*).</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/8_SMA.JPG\" style=\"width: 600px; height: 304px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/8_SMA_Cycle.JPG\" style=\"height:300px; width:705px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/8_SMA_Techniques.JPG\" style=\"height:400px; width:574px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Social-Media-Analytics-Challenges\"><u>Tantangan Social Media Analytics</u></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>\n",
    "\t<p>Pendek (<strong>Short </strong>in lengths): bahkan terkadang tidak mengandung sebuah kalimat yang utuh menurut tata bahasa (grammar).</p>\n",
    "\t</li>\n",
    "\t<li><strong>Noise&nbsp;</strong>: Data media sosial penuh dengan noise seperti typos (salah ketik), encoding yang tidak jamak, slang, dsb.</li>\n",
    "\t<li><strong>Temporal&nbsp;</strong>: Informasi yang sedang trending biasanya hanya sesaat,<br />\n",
    "\tsehingga SMA diharapkan dilakukan dengan cepat menggunakan model-model/teknik-teknik analisa data yang efisien.</li>\n",
    "\t<li><strong>High-dimensional</strong> : Data di Media Sosial (Teks, Gambar, Video, Suara, dsb) adalah data tidak terstruktur berdimensi tinggi.</li>\n",
    "\t<li><strong>Fine-grained</strong> : Data di media sosial berasal dari banyak user yang masing-masingnya bisa jadi membahas beberapa topik yang berbeda.<br />\n",
    "\tSehingga komunitas (kelompok), topik, maupun klasifikasi yang ada menjadi besar (fine-grained).</li>\n",
    "\t<li><strong>Large in volume</strong>&nbsp;&amp; <strong>High velocity</strong>:&nbsp; Data yang sangat besar dan bertambah besar dengan cepat.</li>\n",
    "\t<li><strong>A lot of external Information</strong> : Informasi terkadang lebih banyak terkandung dari luar (eksternal) seperti url website, video, atau hal lain yang dibagikan oleh pengguna media sosial.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Case-Study:-twitter\">Case Study: twitter</h2>\n",
    "<ol>\n",
    "\t<li>API Keys</li>\n",
    "\t<li>Rules</li>\n",
    "\t<li>Crawling by searching</li>\n",
    "    <li>tweet Json</li>\n",
    "\t<li>Crawling by Streaming and Scrapping</li>\n",
    "    <li>twitter Social Media Analytics</li>\n",
    "</ol>\n",
    "<img alt=\"\" src=\"images/6_twitter.png\" style=\"width: 300px; height: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Aturan-twitter\">Aturan, bentuk data, &amp; error codes twitter</h2>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/rest/public/rate-limiting\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/rest/public/rate-limiting\" target=\"_blank\">dev.twitter.com/rest/public/rate-limiting</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/terms/agreement-and-policy\" target=\"_blank\">https://dev.twitter.com/overview/terms/agreement-and-policy</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/api/response-codes\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/overview/api/response-codes\" target=\"_blank\">dev.twitter.com/overview/api/response-codes</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/api/tweets\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/overview/api/tweets\" target=\"_blank\">dev.twitter.com/overview/api/tweets</a></p>\n",
    "\t</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "* https://developer.twitter.com/en/dashboard\n",
    "* https://developer.twitter.com/en/apps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Crawling-Data\">Crawling/Scrapping Data</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Digital_Media_Crawling_.png\" /></p>\n",
    "\n",
    "* Credits, image source: https://www.promptcloud.com/blog/scraping-social-media-data-for-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using Google Colab\n",
    "\n",
    "!pip install tweepy, twython, pyLDAvis, textblob, unidecode, googlemaps\n",
    "\n",
    "U = ['https://raw.githubusercontent.com/taufikedys/tau-data/master/data/slang.dic',\n",
    "    'https://raw.githubusercontent.com/taufikedys/tau-data/master/data/stopwords_id.txt',\n",
    "    'https://raw.githubusercontent.com/taufikedys/tau-data/master/data/stopwords_en.txt',\n",
    "    'https://raw.githubusercontent.com/taufikedys/tau-data/master/data/kata_dasar.txt',\n",
    "    'https://github.com/taufikedys/tau-data/raw/master/data/all_indo_man_tag_corpus_model.crf.tagger']\n",
    "!wget https://raw.githubusercontent.com/taufikedys/tau-data/master/taudata.py\n",
    "!mkdir data\n",
    "for url in U:\n",
    "    !wget -P data/ url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore') \n",
    "import pandas as pd, nltk.classify.util, seaborn as sns#; sns.set()\n",
    "import tweepy, csv, json, time, re, taudata as tau\n",
    "import networkx as nx, operator, numpy as np, hashlib\n",
    "import pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
    "import requests, googlemaps, urllib.request, urllib3, urllib.parse\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from twython import TwythonStreamer\n",
    "from datetime import datetime \n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.element import Comment\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from lxml.html import fromstring\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning  \n",
    "from twython import TwythonStreamer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh API Keys (Sesuaikan dengan API keys masing-masing)\n",
    "# consumer_key, consumer_secret, access_token, access_secret\n",
    "Ck = 'XCazkZphXyWNBhvATCgTABQ6d'\n",
    "Cs = 'Sz8S35MeD5jGaD2cal3ekbnAM9oTbTzvoEMczGbp2X8hcAbVMM'\n",
    "At = '3214118411-3xSIeXA1yTsCh0BOCZ02z2luec5GxTPR855RtdU'\n",
    "As = 'sgXwdbKAetqIg54Hp9w9Qp8bCAPAHpT4FPl0AKEOseF0I'\n",
    "gKey = 'BIzaSyBjW1wJtX8ca_KrpzIvL8mWZJcsEBU-fZY'\n",
    "fSlang = 'data/slang.dic'\n",
    "dataPath = 'data/'\n",
    "stopId, lemmaId = tau.LoadStopWords(lang='id')\n",
    "slangFixId = tau.loadCorpus(file = fSlang, sep=':')\n",
    "gmaps = googlemaps.Client(key=gKey)\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Koneksi ke server twitter\n",
    "twitter = tau.twitter_connect(Ck, Cs, At, As)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'data'\n",
    "Tweets = tau.getTweets(twitter, topic, N = 300, lan='id')\n",
    "posts = [(t['user']['screen_name'], t['full_text']) for t in Tweets]\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk tweet = Json. Contoh tweet pertama:\n",
    "Tweets[0]['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengakses data spesifik pada tweet yang pertama:\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(Tweets[0]['user']['screen_name'],Tweets[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan hasil crawling twitter\n",
    "ftopic = topic.replace('\"','-').replace(' ','_')\n",
    "fileName = 'data/'+ftopic+'_Tweets.json'\n",
    "tau.saveTweets(Tweets,file=fileName)\n",
    "print('Saved to '+fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me-load kembali jika (misal) analisa ingin dilakukan di lain waktu\n",
    "# Sengaja nama variabelnya saya bedakan (T2) untuk menunjukkan loading data berhasil\n",
    "T2 = tau.loadTweets(file=fileName)\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(T2[0]['user']['screen_name'],T2[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengambil hanya data tweet - Untuk meudian dilakukan Text Mining\n",
    "D = [t['full_text'] for t in Tweets]\n",
    "D[:5] # 5 tweet pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreProcessing/cleaning dilakukan dengan cara yang sama dengan sebelumnya\n",
    "for i,d in tqdm(enumerate(D)):\n",
    "    D[i] = tau.cleanText(d, fix=slangFixId, stops=stopId)\n",
    "print(D[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Pemilihan-KeyWords\">Pemilihan KeyWords untuk Data Mining</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li><a href=\"https://medium.com/lingvo-masino/how-to-choose-keywords-in-twitter-9c3b85c50290\" target=\"_blank\">https://medium.com/lingvo-masino/how-to-choose-keywords-in-twitter-9c3b85c50290</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Operator (penting)\n",
    "\n",
    "<ul>\n",
    "\t<li><img alt=\"\" src=\"images/query_Operator.png\" style=\"width: 661px; height: 554px;\" /></li>\n",
    "    <li>Detail: <a href=\"https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html\" target=\"_blank\">https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita coba #1\n",
    "topic = ' from:jokowi'\n",
    "T = tau.getTweets(twitter, topic, N = 7, lan='id') # lan = 'id' atau 'en' ...\n",
    "[t['full_text'] for t in T]\n",
    "# Lesson ... becarefull with duplicates ==> use Hash Function!!!... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita coba #2\n",
    "topic = 'kuliah :)'\n",
    "T = tau.getTweets(twitter, topic, N = 7) # lan = 'id' atau 'en' ...\n",
    "[t['full_text'] for t in T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Streaming-Data\">Streaming and Scrapping Data</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Meme_Streaming_Data.jpg\" style=\"width: 307px; height: 309px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming tweets. Untuk percobaan pilih topicS sesuatu yg sedang trending/populer \"saat ini\".\n",
    "# Atau bisa coba dengan mengirim tweet sendiri :)\n",
    "\n",
    "def streamTwitter(topicS, lang):\n",
    "    class MyStreamer(TwythonStreamer):\n",
    "        def on_success(self, data):\n",
    "            global count\n",
    "            count+=1\n",
    "            print('tweet from {}, post: {}'.format(data['user']['screen_name'], data['text']))\n",
    "            if count==maxTweet:\n",
    "                print('\\nFinished streaming %.0f tweets' %(maxTweet)); self.disconnect()\n",
    "        def on_error(self, status_code, data):\n",
    "            print('Error Status = %s' %status_code); self.disconnect()\n",
    "\n",
    "    while count<maxTweet:\n",
    "        stream = MyStreamer(Ck, Cs, At, As)\n",
    "        stream.statuses.filter(track=topicS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweet, count = 3, 0 # Rubah sesuai dengan kebutuhan, Untuk percobaan ini cukup (misal) 3 tweet\n",
    "lang = set(['en','id']) # bahasa bisa dipilih > 1\n",
    "topicS = ['data'] # Bisa>1\n",
    "\n",
    "streamTwitter(topicS, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping (going beyond 7 days)\n",
    "\n",
    "## https://tau-data.id/crawling-twitter/\n",
    "### Solusi bagi yang belum memiliki API keys\n",
    "### Wajib menggunakan browser FireFox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengubah html hasil scrapping ke CSV\n",
    "fileSource = 'data/contoh_scrap.htm'\n",
    "fileOutput = 'data/contoh_scrap.csv'\n",
    "\n",
    "tau.twitter_html2csv(fileSource, fileOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualisasi ~ Simple Text Analytics</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak seperti data terstruktur, data tidak terstruktur seperti teks termasuk salah satu data yang cukup sulit untuk divisualisasikan.<br />\n",
    "\t<img alt=\"\" src=\"images/11_charts.jpg\" style=\"height:150px; width:276px\" /></li>\n",
    "\t<li>Namun terdapat Tools seperti Voyant yang dapat membantu dalam visualisasi sekaligus analisis.<br />\n",
    "\t<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /></li>\n",
    "    <li><a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-1:-WordClouds\">Penggunaan Voyant 1: WordClouds</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.</li>\n",
    "\t<li>slider terms: mengkontrol banyaknya terms yang disertakan.</li>\n",
    "\t<li><strong>Summary </strong>(statistics)</li>\n",
    "\t<li><strong>Documents </strong>==&gt; add more</li>\n",
    "\t<li><strong>Phrases </strong>(n-grams like)</li>\n",
    "\t<li><strong>Export </strong>Visualisasi (kanan atas - pertama)</li>\n",
    "\t<li><strong>Options </strong>(kanan atas ke-3): Font, size, stopwords, whitelist</li>\n",
    "\t<li>&quot;?&quot; ==&gt; More Help</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 2: Bubbles</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>&nbsp;Klik tanda 4-kotak kecil (kanan atas ke-3)</li>\n",
    "\t<li>Pilih Visualization Tools ==&gt; Bubbles</li>\n",
    "\t<li>Option: hanya stopwords</li>\n",
    "\t<li>Export: Hanya PNG</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-3:-Word-Tree\">Penggunaan Voyant 3: Word Tree</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Klik branch untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 4: Links</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Visualization Tools ==&gt; Links</li>\n",
    "\t<li>Klik sembarang terms untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-5:-Trends\">Penggunaan Voyant 5: Trends</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Document Tools ==&gt; Trends</li>\n",
    "\t<li>.. Butuh preprocessing ...&nbsp;</li>\n",
    "\t<li>Data harus terurut waktu</li>\n",
    "\t<li>Berikut contohnya</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_Sentiment_Analysis_Meme.jpg\" style=\"height:300px; width:400px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Apakah sentiment analysis?</strong></p>\n",
    "\n",
    "* Sentiment analysis, also called opinion mining, is the field of study that analyzes people’s opinions, sentiments, appraisals, attitudes, and emotions toward entities and their attributes expressed in written text [Bing Liu 2014].\n",
    "\n",
    "<p>Terkadang disebut juga sebagai&nbsp;<strong>opinion mining.</strong> (walau technically sebenarnya berbeda)</p>\n",
    "\n",
    "<p><strong>Contoh aplikasi Sentiment Analysis</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li><strong>Business</strong>: tanggapan konsumen atas suatu produk.</li>\n",
    "\t<li><strong>Politics</strong>: Sentimen masyarakat sebagai strategi pemenangan pemilu/pilkada.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "For proper definition see:\n",
    "* Liu, B., 2015. Sentiment analysis: Mining opinions, sentiments, and emotions. Cambridge University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_SA_techniques.jpg\" style=\"height:300px; width:536px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# Lexicon Based berdasarkan \n",
    "# pattern = https://www.clips.uantwerpen.be/pages/pattern-en#sentiment\n",
    "Sentence = \"Bakpia is not delicious, I don't like it\"\n",
    "testimonial = TextBlob(Sentence)\n",
    "print(testimonial.sentiment)\n",
    "print('Polarity=Sentimen =', testimonial.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana Dengan Bahasa Indonesia?\n",
    "\n",
    "* A \"simple\" trick\n",
    "* Lexicon and Machine Translation based Method:\n",
    "Bojar, O., & Veselovská, K. (2015). Resources for Indonesian Sentiment Analysis. The Prague Bulletin of Mathematical Linguistics, 103(1), 21-41.\n",
    "* https://ufal.mff.cuni.cz/pbml/103/art-franky-bojar-veselovska.pdf?fbclid=IwAR3_Ojz2oF_ezH7IHb6iqMxN3ouIIBlDDyC6QBEO7455niVDQvzfbIjKlm0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SenSubModMood_ID(kalimat):\n",
    "    K = TextBlob(kalimat).translate(to='en')\n",
    "    pol,sub = K.sentiment\n",
    "    if pol>0:\n",
    "        pol='positive'\n",
    "    elif pol<0:\n",
    "        pol='negative'\n",
    "    else:\n",
    "        pol = 'netral'\n",
    "    if sub>0.5:\n",
    "        sub = 'Subjektif'\n",
    "    else:\n",
    "        sub = \"Objektif\"\n",
    "    return pol, sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = 'makan bakpia pakai kecap enak'\n",
    "SenSubModMood_ID(kalimat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana dengan Sentiment Analysis menggunakan NBC untuk Bahasa indonesia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "def bentukClassifier(P,Ne,Nt):\n",
    "    positive_features = [(word_feats(pos), 'pos') for pos in P]\n",
    "    negative_features = [(word_feats(neg), 'neg') for neg in Ne]\n",
    "    neutral_features = [(word_feats(neu), 'neu') for neu in Nt]\n",
    "    train_set = negative_features + positive_features + neutral_features\n",
    "    return NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "def prediksiSentiment(sentence, model):\n",
    "    pos, neg = 0, 0\n",
    "    words = sentence.lower().split(' ')\n",
    "    for word in words:\n",
    "        classResult = model.classify( word_feats(word))\n",
    "        if classResult == 'neg':\n",
    "            neg = neg + 1\n",
    "        if classResult == 'pos':\n",
    "            pos = pos + 1\n",
    "    if pos>neg:\n",
    "        return 'positive'\n",
    "    elif pos==neg:\n",
    "        return 'netral'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [ 'keren', 'suka', 'cinta', 'bagus', 'mantap', 'sadis', 'top', ':)' ]\n",
    "Ne = [ 'jelek', 'benci','buruk', 'najis', ':(']\n",
    "Nt = [ 'bakso','film','pisang','pagi','makan','kopi','minum','sambil','abis']\n",
    "\n",
    "sentence = \"makan pempek minumnya teh panas, mantap sekali\"\n",
    "model = bentukClassifier(P,Ne,Nt)\n",
    "prediksiSentiment(sentence,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana jika mau melakukannya dengan model klasifikasi (supervised learning) lain seperti modul sebelumnya?\n",
    "(e.g. SVM, NN, DT, k-NN, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text Classification : independent variable\n",
    "d1 = 'Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget'\n",
    "d3 = 'Palembang agak mendung hari ini'\n",
    "d4 =  'Sudah lumayan lama tukang Bakso belum lewat'\n",
    "d5 = 'Aduh ga banget makan Mie Ayam pakai kecap, please deh'\n",
    "d6 = 'Benci banget kalau melihat orang buang sampah sembarangan di jalan'\n",
    "d7 = 'Kalau liat orang ga taat aturan rasanya ingin ngegampar aja'\n",
    "d8 = 'Nikmatnya meniti jalan jalan penuh romansa di tengah kota bernuansa pendidikan'\n",
    "d9 = 'kemajuan bangsa ini ada pada kegigihan masyarakat dalam belajar dan bekerja'\n",
    "D = [d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent variable, misal 0=positif, 1=netral, 2=negatif\n",
    "Class = [1,1,0,0,-1,-1,-1,0,1]\n",
    "dic = {-1:'negatif', 0:'netral', 1:'positif'}\n",
    "print([dic[c] for c in Class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk VSM-nya seperti kemarin (skip preprocessing)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = vectorizer.fit_transform(D)\n",
    "vsm = vsm[vsm.getnnz(1)>0][:,vsm.getnnz(0)>0] # Remove zero rows and columns\n",
    "print(vsm.shape)\n",
    "str(vectorizer.vocabulary_)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan klasifikasi (misal dengan SVM)\n",
    "dSVM = svm.SVC(kernel='linear')\n",
    "sen = dSVM.fit(vsm, Class).predict(vsm)\n",
    "print(accuracy_score(Class, sen))\n",
    "# Memakai seluruh training data karena sampel yang sangat kecil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Supplementary\">Supplementary</h2>\n",
    "\n",
    "<p>* Negasi suatu kata bukan berarti memiliki sentimen kebalikannya. Misal &quot;jelek&quot; dan &quot;tidak jelek&quot; (terrible vs not terrible).</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/negation_sentiments.png\" /></p>\n",
    "\n",
    "<p>[*]. Zhu, X., Guo, H., Mohammad, S., &amp; Kiritchenko, S. (2014). An empirical study on the effect of negation words on sentiment. In&nbsp;<i>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>&nbsp;(Vol. 1, pp. 304-313).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Makna positive/negative atau pro/kontra subjective (bias) terhadap user.\n",
    "* StopWords removal in general is a bad idea\n",
    "* learn the lingo in your topic, sentiment expressions are different across fields, languages, and regions.\n",
    "* Sarcasm perlu konteks untuk di deteksi dengan tepat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Feature-Engineering/Extraction\">Feature Engineering/Extraction</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Ketimbang pemilihan model yang optimal, beberapa literature sudah melaporkan bahwa feature engineering/extraction lebih efektif [1].</li>\n",
    "\t<li>Selain itu, pendekatan semantic dalam FE juga lebih plausible untuk dilakukan.</li>\n",
    "\t<li>Tabel berikut adalah contoh FE yang bisa dilakukan spesifik terhadap model SA.</li>\n",
    "\t<li><img alt=\"\" src=\"images/SA_Analysis_Features.png\" style=\"width: 544px; height: 425px;\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ketika mengolah dokumen (file dalam bentuk teks), harapan kita seperti ini:</h3>\n",
    "\n",
    "<img alt=\"\" src=\"images/4_harapan_LSA.png\" style=\"height:99px; width:198px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Namun kita sudah bahas kemarin kenyataannya seperti ini:</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/4_kenyataan_LSA.png\" style=\"height:183px; width:182px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Topic-Modelling-1-:-Latent-Dirichlet-Allocation\">Topic Modelling 1 : Latent Dirichlet Allocation</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/4_Document_to_Topics.png\" style=\"height: 300px ; width: 582px\" /></p>\n",
    "\n",
    "<p><strong><big>Tapi bukan seperti klasifikasi dan bukan berarti kata-kata Sport, Technology, dan Entertainment dominan di kategori-kategori tersebut. Topic modelling lebih ke soft-clustering, dimana suatu dokumen dimasukkan ke dalam beberapa cluster (topic) sekaligus. Adapun nama &quot;topic/cluster&quot;-nya di interpretasi dari kata-kata yang ada didalamnya.</big></strong></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/4_LDA vs LDA.JPG\" style=\"height:400px; width:606px\" /></p>\n",
    "[<a href=\"http://chdoig.github.io/pytexas2015-topic-modeling/\" target=\"_blank\">Sumber gambar ini dan beberapa gambar selanjutnya</a>]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/4_definisi topic model.JPG\" style=\"height:350px; width:809px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/4_inti_LDA.JPG\" style=\"height:500px; width:785px\" /></p>\n",
    "Penjelasan intuitif yang baik: https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/4_LDA Pipeline.JPG\" style=\"height:300px; width:663px\" /></p>\n",
    "* Modifikasi dapat dilakukan dengan \"pos tags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita mulai dengan membuat VSM-nya\n",
    "# kita gunakan perintah yang ada di Segmen sebelumnya \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer()\n",
    "\n",
    "D = [t['full_text'] for t in Tweets]\n",
    "for i,d in tqdm(enumerate(D)):\n",
    "    D[i] = tau.cleanText(d, fix=slangFixId, stops=stopId)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(D)\n",
    "tf_terms = tf_vectorizer.get_feature_names()\n",
    "# Mengapa tf bukan tfidf?\n",
    "# Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.\n",
    "# Saran: untuk penelitian VS di Industri.\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dilanjutkan dengan membentuk model LDA-nya\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "n_topics = 3\n",
    "lda = LDA(n_components=n_topics, learning_method='batch', random_state=0).fit(tf)   \n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat Topik-topiknya\n",
    "vsm_topics = lda.transform(tf)\n",
    "print(vsm_topics.shape)\n",
    "vsm_topics[:10]\n",
    "# Ukuran kolom = #Topics ==> Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita coba maknai masing-masing topic ini\n",
    "Top_Words=7\n",
    "print('Printing top {0} Topics, with top {1} Words:'.format(n_topics, Top_Words))\n",
    "tau.print_Topics(lda, tf_terms, n_topics, Top_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# Mari kita Plot, supaya lebih jelas\n",
    "# Catatan, bergantung dari laptop yang digunakan, image terkadang cukup lama untuk muncul.\n",
    "import pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "<h3 id=\"Non-Negative-Matrix-Decomposition-(NMF)\">Non-Negative Matrix Decomposition (NMF)</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/4_NMF.jpg\" style=\"height: 349px; width: 600px;\" /> [image source: <a href=\"https://www.slideshare.net/SebastianRuder/dynamic-topic-modeling-via-nonnegative-matrix-factorization-dr-derek-greene]\">https://www.slideshare.net/SebastianRuder/dynamic-topic-modeling-via-nonnegative-matrix-factorization-dr-derek-greene]</a></p>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Tujuan-NMF:\">Tujuan NMF:</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/4_NMF_Goal.JPG\" style=\"height: 363px; width: 600px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsung Aplikasi-nya\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(D)\n",
    "nmf_model = NMF(n_components = 3, random_state=0)\n",
    "nmf = nmf_model.fit(tfidf)\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tau.print_Topics(nmf, tfidf_feature_names, n_topics, Top_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sama seperti LDA kita bisa melihat distribusi topic setiap dokumen\n",
    "vsm_topics = nmf.transform(tfidf)\n",
    "vsm_topics[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Graph-From-Social-media\">Graph From Social media</h1>\n",
    "\n",
    "<h3 id=\"Mentions,-Followers,-Friends\">Mentions, Followers, Friends</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/SNA_Graph_Types.png\" style=\"width: 700px; height: 443px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the Tweet Graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(12,8))\n",
    "users = [t['user']['screen_name'] for t in Tweets]\n",
    "tweets = [t['full_text'] for t in Tweets]\n",
    "G = tau.Graph([users,tweets], Label = False, layOut='spring', plain=True) # layOut = spring, circular, random, shells, spectral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>I. Centrality&nbsp;Analysis</h2>\n",
    "\n",
    "<p>Bertujuan untuk menemukan pengguna yang paling berpengaruh dalam suatu topik pembicaraan di media sosial. Analisanya biasanya dilakukan melalui data graph dari hubungan jaringan pertemanan (follower/friend) antar pengguna atau komunikasi antar pengguna (mentions).</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/8_SMA_Centrality.JPG\" style=\"height: 400px ; width: 600px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Centrality-by-Degree\">Centrality by Degree</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Degree_Centrality.png\" style=\"width: 800px; height: 505px;\" /></p>\n",
    "\n",
    "## Apakah interpretasinya?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now examine, who are the most \"important\" users in this Graph?\n",
    "Gt = tau.Centrality(G, N=10, method='degree', outliers=False, Label = True, layOut='spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau.drawGraph(Gt, True, layOut='circular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Closeness-Centrality\">Closeness Centrality</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/closeness_centrality.png\" style=\"width: 700px; height: 320px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gt = tau.Centrality(G, N=10, method='closeness', outliers=False, Label = True, layOut='spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau.drawGraph(Gt, True, layOut='circular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Betweenness-Centrality\">Betweenness Centrality</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/betweeness_Centrality.png\" style=\"width: 700px; height: 368px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gt = tau.Centrality(G, N=10, method='betweeness', outliers=False, Label = True, layOut='spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau.drawGraph(Gt, True, layOut='circular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvector Centrality\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Eigenvector_Centrality_1.png\" style=\"width: 685px; height: 430px;\" /></p>\n",
    "\n",
    "### Digunakan juga oleh Google dalam Algoritma PageRank-nya untuk menentukan halaman web terpenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/Eigenvector_Centrality_0_.png\" style=\"width: 685px; height: 430px;\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gt = tau.Centrality(G, N=10, method='eigen', outliers=False, Label = True, layOut='spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau.drawGraph(Gt, True, layOut='circular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Summary\">Summary</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Centrality_Interpretations.png\" style=\"width: 659px; height: 349px;\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality dapat digunakan untuk membuat visualisasi graph yang lebih baik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,8))\n",
    "\n",
    "g = nx.karate_club_graph()\n",
    "\n",
    "pos = nx.spring_layout(g) # Spring LayOut\n",
    "nx.draw_networkx_nodes(g,pos, alpha=0.2,node_color='blue',node_size=600) # Gambar Vertex\n",
    "nx.draw_networkx_edges(g,pos,width=2,alpha=0.1) # Gambar edges\n",
    "nx.draw_networkx_labels(g,pos) #Gambar Label Nodes\n",
    "plt.show() # Show the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan Centrality measure (misal degree) untuk merubah ukuran node\n",
    "plt.subplots(figsize=(12,8))\n",
    "\n",
    "\n",
    "K = 100 # Scale factor\n",
    "d = nx.degree(g) \n",
    "d = [d[node]*K for node in g.nodes()]\n",
    "\n",
    "pos = nx.spring_layout(g) # Spring LayOut\n",
    "nx.draw_networkx_nodes(g,pos,node_size=d) # Gambar Vertex\n",
    "nx.draw_networkx_edges(g,pos,width=2,alpha=0.15) # Gambar edges\n",
    "nx.draw_networkx_labels(g,pos) #Gambar Label Nodes\n",
    "plt.show() # Show the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan tingkat \"kepentingan\" sebagai warna\n",
    "ranking = nx.degree_centrality(g)\n",
    "warna = list(ranking.values())\n",
    "print(warna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,8))\n",
    "\n",
    "pos = nx.spring_layout(g) # Spring LayOut\n",
    "nx.draw_networkx_nodes(g,pos, node_color=warna,node_size=d) # Gambar Vertex\n",
    "nx.draw_networkx_edges(g,pos,width=2,alpha=0.1) # Gambar edges\n",
    "nx.draw_networkx_labels(g,pos) #Gambar Label Nodes\n",
    "plt.show() # Show the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bipartition (Bisection) Partitioning\n",
    "\n",
    "* <img alt=\"\" src=\"img/KL_Algorithms.png\" />\n",
    "* This algorithm paritions a network into two sets by iteratively swapping pairs of nodes to reduce the edge cut between the two sets.\n",
    "* https://www.youtube.com/watch?v=MMlf66PQdN8\n",
    "* Paper: Kernighan, B. W.; Lin, Shen (1970). “An efficient heuristic procedure for partitioning graphs.” Bell Systems Technical Journal 49: 291–307. Oxford University Press 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nx.algorithms.community.kernighan_lin_bisection(g)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warna = []\n",
    "for v in B[0]:\n",
    "    warna.append(1)\n",
    "for v in B[1]:\n",
    "    warna.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.shell_layout(g, B)\n",
    "nx.draw_networkx_nodes(g,pos, node_color=warna,node_size=d) # Gambar Vertex\n",
    "nx.draw_networkx_edges(g,pos,width=2,alpha=0.1) # Gambar edges\n",
    "nx.draw_networkx_labels(g,pos) #Gambar Label Nodes\n",
    "plt.show() # Show the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/Partitioning_VS_Clustering_Graph.png\" style=\"width: 800px; height: 428px;\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Clustering via \"Modularity\"\n",
    "\n",
    "* Modules biasa disebut juga groups, clusters atau communities\n",
    "* Terdapat berbagai cara dalam menghitung \"Modularity\" (contoh dibawah)\n",
    "* Graph with high modularity have dense connections between the nodes within \"modules\" but sparse connections between nodes in different modules.\n",
    "* Salah satu metodenya : Greedy Modularity Maximization (GMM)\n",
    "* GMM begins with each node in its own community and joins the pair of communities that most increases modularity until no such pair exists.\n",
    "* Clauset, A., Newman, M. E., & Moore, C. “Finding community structure in very large networks.” Physical Review E 70(6), 2004.\n",
    "* Other resources for study: https://slideplayer.com/slide/7050174/\n",
    "\n",
    "<img alt=\"\" src=\"img/Graph_Modularity.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!!!... Hanya bisa jika networkX versi 2.2 ke atas \n",
    "M = nx.algorithms.community.greedy_modularity_communities(g)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = []\n",
    "warna = 1\n",
    "for module in M:\n",
    "    for node in module:\n",
    "        W.append(warna)\n",
    "    warna = warna +1\n",
    "print(W)\n",
    "\n",
    "K = 100 # Scale factor\n",
    "d = nx.degree(g) \n",
    "d = [d[node]*K for node in g.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,8))\n",
    "\n",
    "\n",
    "pos = nx.shell_layout(g, M)\n",
    "nx.draw_networkx_nodes(g,pos, node_color=W,node_size=d) # Gambar Vertex\n",
    "nx.draw_networkx_edges(g,pos,width=2,alpha=0.1) # Gambar edges\n",
    "nx.draw_networkx_labels(g,pos) #Gambar Label Nodes\n",
    "plt.show() # Show the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penjelasan & Code yang lebih lengkap tentang Centrality & Community Analysis\n",
    "\n",
    "https://tau-data.id/community-detection-centrality/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Module \n",
    "\n",
    "<hr />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
